{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 图像分类\n",
    "\n",
    "在此项目中，你将对 [CIFAR-10 数据集](https://www.cs.toronto.edu/~kriz/cifar.html) 中的图片进行分类。该数据集包含飞机、猫狗和其他物体。你需要预处理这些图片，然后用所有样本训练一个卷积神经网络。图片需要标准化（normalized），标签需要采用 one-hot 编码。你需要应用所学的知识构建卷积的、最大池化（max pooling）、丢弃（dropout）和完全连接（fully connected）的层。最后，你需要在样本图片上看到神经网络的预测结果。\n",
    "\n",
    "\n",
    "## 获取数据\n",
    "\n",
    "请运行以下单元，以下载 [CIFAR-10 数据集（Python版）](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 探索数据\n",
    "\n",
    "该数据集分成了几部分／批次（batches），以免你的机器在计算时内存不足。CIFAR-10 数据集包含 5 个部分，名称分别为 `data_batch_1`、`data_batch_2`，以此类推。每个部分都包含以下某个类别的标签和图片：\n",
    "\n",
    "* 飞机\n",
    "* 汽车\n",
    "* 鸟类\n",
    "* 猫\n",
    "* 鹿\n",
    "* 狗\n",
    "* 青蛙\n",
    "* 马\n",
    "* 船只\n",
    "* 卡车\n",
    "\n",
    "了解数据集也是对数据进行预测的必经步骤。你可以通过更改 `batch_id` 和 `sample_id` 探索下面的代码单元。`batch_id` 是数据集一个部分的 ID（1 到 5）。`sample_id` 是该部分中图片和标签对（label pair）的 ID。\n",
    "\n",
    "问问你自己：“可能的标签有哪些？”、“图片数据的值范围是多少？”、“标签是按顺序排列，还是随机排列的？”。思考类似的问题，有助于你预处理数据，并使预测结果更准确。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fef193f8630>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现预处理函数\n",
    "\n",
    "### 标准化\n",
    "\n",
    "在下面的单元中，实现 `normalize` 函数，传入图片数据 `x`，并返回标准化 Numpy 数组。值应该在 0 到 1 的范围内（含 0 和 1）。返回对象应该和 `x` 的形状一样。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meirtz/anaconda3/envs/dl_dev/lib/python3.5/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by the normalize function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing \n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x_len = x.shape[0]\n",
    "    # print (np.reshape(preprocessing.normalize(np.reshape(x, (x_len, -1))), (x_len, 32, 32, 3)))\n",
    "    return np.reshape(preprocessing.normalize(np.reshape(x, (x_len, -1))), (x_len, 32, 32, 3))\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot 编码\n",
    "\n",
    "和之前的代码单元一样，你将为预处理实现一个函数。这次，你将实现 `one_hot_encode` 函数。输入，也就是 `x`，是一个标签列表。实现该函数，以返回为 one_hot 编码的 Numpy 数组的标签列表。标签的可能值为 0 到 9。每次调用 `one_hot_encode` 时，对于每个值，one_hot 编码函数应该返回相同的编码。确保将编码映射保存到该函数外面。\n",
    "\n",
    "提示：不要重复发明轮子。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement Function\n",
    "    y = np.zeros([len(x), 10])\n",
    "    for i,j in enumerate(x):\n",
    "        y[i, j] = 1\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机化数据\n",
    "\n",
    "之前探索数据时，你已经了解到，样本的顺序是随机的。再随机化一次也不会有什么关系，但是对于这个数据集没有必要。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理所有数据并保存\n",
    "\n",
    "运行下方的代码单元，将预处理所有 CIFAR-10 数据，并保存到文件中。下面的代码还使用了 10% 的训练数据，用来验证。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meirtz/anaconda3/envs/dl_dev/lib/python3.5/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype uint8 was converted to float64 by the normalize function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检查点\n",
    "\n",
    "这是你的第一个检查点。如果你什么时候决定再回到该记事本，或需要重新启动该记事本，你可以从这里开始。预处理的数据已保存到本地。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建网络\n",
    "\n",
    "对于该神经网络，你需要将每层都构建为一个函数。你看到的大部分代码都位于函数外面。要更全面地测试你的代码，我们需要你将每层放入一个函数中。这样使我们能够提供更好的反馈，并使用我们的统一测试检测简单的错误，然后再提交项目。\n",
    "\n",
    ">**注意**：如果你觉得每周很难抽出足够的时间学习这门课程，我们为此项目提供了一个小捷径。对于接下来的几个问题，你可以使用 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 程序包中的类来构建每个层级，但是“卷积和最大池化层级”部分的层级除外。TF Layers 和 Keras 及 TFLearn 层级类似，因此很容易学会。\n",
    "\n",
    ">但是，如果你想充分利用这门课程，请尝试自己解决所有问题，不使用 TF Layers 程序包中的任何类。你依然可以使用其他程序包中的类，这些类和你在 TF Layers 中的类名称是一样的！例如，你可以使用 TF Neural Network 版本的 `conv2d` 类 [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)，而不是 TF Layers 版本的 `conv2d` 类 [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d)。\n",
    "\n",
    "我们开始吧！\n",
    "\n",
    "\n",
    "### 输入\n",
    "\n",
    "神经网络需要读取图片数据、one-hot 编码标签和丢弃保留概率（dropout keep probability）。请实现以下函数：\n",
    "\n",
    "* 实现 `neural_net_image_input`\n",
    " * 返回 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * 使用 `image_shape` 设置形状，部分大小设为 `None`\n",
    " * 使用 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) 中的 TensorFlow `name` 参数对 TensorFlow 占位符 \"x\" 命名\n",
    "* 实现 `neural_net_label_input`\n",
    " * 返回 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * 使用 `n_classes` 设置形状，部分大小设为 `None`\n",
    " * 使用 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) 中的 TensorFlow `name` 参数对 TensorFlow 占位符 \"y\" 命名\n",
    "* 实现 `neural_net_keep_prob_input`\n",
    " * 返回 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)，用于丢弃保留概率\n",
    " * 使用 [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) 中的 TensorFlow `name` 参数对 TensorFlow 占位符 \"keep_prob\" 命名\n",
    "\n",
    "这些名称将在项目结束时，用于加载保存的模型。\n",
    "\n",
    "注意：TensorFlow 中的 `None` 表示形状可以是动态大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    return tf.placeholder(tf.float32, [None, image_shape[0], image_shape[1], image_shape[2]], name=\"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, [None, n_classes], name=\"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积和最大池化层\n",
    "\n",
    "卷积层级适合处理图片。对于此代码单元，你应该实现函数 `conv2d_maxpool` 以便应用卷积然后进行最大池化：\n",
    "\n",
    "* 使用 `conv_ksize`、`conv_num_outputs` 和 `x_tensor` 的形状创建权重（weight）和偏置（bias）。\n",
    "* 使用权重和 `conv_strides` 对 `x_tensor` 应用卷积。\n",
    " * 建议使用我们建议的间距（padding），当然也可以使用任何其他间距。\n",
    "* 添加偏置\n",
    "* 向卷积中添加非线性激活（nonlinear activation）\n",
    "* 使用 `pool_ksize` 和 `pool_strides` 应用最大池化\n",
    " * 建议使用我们建议的间距（padding），当然也可以使用任何其他间距。\n",
    "\n",
    "**注意**：对于**此层**，**请勿使用** [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers)，但是仍然可以使用 TensorFlow 的 [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) 包。对于所有**其他层**，你依然可以使用快捷方法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #height，weight，depth，numbers\n",
    "    channel = x_tensor.get_shape().as_list()[-1]\n",
    "    my_conv = tf.layers.conv2d(\n",
    "        inputs=x_tensor,\n",
    "        filters=conv_num_outputs,\n",
    "        kernel_size=conv_ksize,\n",
    "        strides=conv_strides,\n",
    "        padding='same',\n",
    "        data_format='channels_last',\n",
    "        dilation_rate=(1, 1),\n",
    "        activation=tf.nn.relu,\n",
    "        use_bias=True,\n",
    "        kernel_initializer=None,\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        kernel_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        activity_regularizer=None,\n",
    "        trainable=True,\n",
    "        name=None,\n",
    "        reuse=None\n",
    "    )\n",
    "    \n",
    "    '''\n",
    "    channel = x_tensor.get_shape().as_list()[-1]\n",
    "    w = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], channel, conv_num_outputs], stddev=0.1))\n",
    "    b = tf.Variable(tf.zeros([conv_num_outputs]))\n",
    "    \n",
    "    cstrides = [1, conv_strides[0], conv_strides[1], 1]\n",
    "    cksize = [1, conv_ksize[0], conv_ksize[1], 1]\n",
    "    pstrides = [1, pool_strides[0], pool_strides[1], 1]\n",
    "    pksize = [1, pool_ksize[0], pool_ksize[1], 1]\n",
    "    padding = \"SAME\"\n",
    "    \n",
    "    x_tensor = tf.nn.conv2d(x_tensor, w, strides=cstrides, padding=padding)\n",
    "    x_tensor = tf.nn.bias_add(x_tensor, b)\n",
    "    x_tensor = tf.nn.relu(x_tensor)\n",
    "    x_tensor = tf.nn.max_pool(x_tensor, ksize=pksize, strides=pstrides, padding=padding)\n",
    "    return x_tensor\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    return tf.layers.max_pooling2d(inputs=my_conv, pool_size=pool_ksize, strides=pool_strides)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 扁平化层\n",
    "\n",
    "实现 `flatten` 函数，将 `x_tensor` 的维度从四维张量（4-D tensor）变成二维张量。输出应该是形状（*部分大小（Batch Size）*，*扁平化图片大小（Flattened Image Size）*）。快捷方法：对于此层，你可以使用 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 包中的类。如果你想要更大挑战，可以仅使用其他 TensorFlow 程序包。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完全连接的层\n",
    "\n",
    "实现 `fully_conn` 函数，以向 `x_tensor` 应用完全连接的层级，形状为（*部分大小（Batch Size）*，*num_outputs*）。快捷方法：对于此层，你可以使用 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 包中的类。如果你想要更大挑战，可以仅使用其他 TensorFlow 程序包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出层\n",
    "\n",
    "实现 `output` 函数，向 x_tensor 应用完全连接的层级，形状为（*部分大小（Batch Size）*，*num_outputs*）。快捷方法：对于此层，你可以使用 [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) 或 [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) 包中的类。如果你想要更大挑战，可以仅使用其他 TensorFlow 程序包。\n",
    "\n",
    "**注意**：该层级不应应用 Activation、softmax 或交叉熵（cross entropy）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    channel = x_tensor.get_shape().as_list()[-1]\n",
    "    w = tf.Variable(tf.truncated_normal([channel, num_outputs], stddev = 0.1))\n",
    "    b = tf.Variable(tf.truncated_normal([num_outputs]))\n",
    "    x_tensor = tf.matmul(x_tensor, w) + b\n",
    "    return x_tensor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建卷积模型\n",
    "\n",
    "实现函数 `conv_net`， 创建卷积神经网络模型。该函数传入一批图片 `x`，并输出对数（logits）。使用你在上方创建的层创建此模型：\n",
    "\n",
    "* 应用 1、2 或 3 个卷积和最大池化层（Convolution and Max Pool layers）\n",
    "* 应用一个扁平层（Flatten Layer）\n",
    "* 应用 1、2 或 3 个完全连接层（Fully Connected Layers）\n",
    "* 应用一个输出层（Output Layer）\n",
    "* 返回输出\n",
    "* 使用 `keep_prob` 向模型中的一个或多个层应用 [TensorFlow 的 Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    net = conv2d_maxpool(x, 20, (6,6), (1,1), (2,2), (1,1))\n",
    "    net = tf.nn.dropout(net, keep_prob)\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    net = flatten(net)\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    net = fully_conn(net, 20)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    net = output(net, 10)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return net\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练神经网络\n",
    "\n",
    "### 单次优化\n",
    "\n",
    "实现函数 `train_neural_network` 以进行单次优化（single optimization）。该优化应该使用 `optimizer` 优化 `session`，其中 `feed_dict` 具有以下参数：\n",
    "\n",
    "* `x` 表示图片输入\n",
    "* `y` 表示标签\n",
    "* `keep_prob` 表示丢弃的保留率\n",
    "\n",
    "每个部分都会调用该函数，所以 `tf.global_variables_initializer()` 已经被调用。\n",
    "\n",
    "注意：不需要返回任何内容。该函数只是用来优化神经网络。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={\n",
    "        keep_prob: keep_probability, \n",
    "        x: feature_batch, \n",
    "        y: label_batch})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 显示数据\n",
    "\n",
    "实现函数 `print_stats` 以输出损失和验证准确率。使用全局变量 `valid_features` 和 `valid_labels` 计算验证准确率。使用保留率 `1.0` 计算损失和验证准确率（loss and validation accuracy）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    loss = session.run(cost, feed_dict={\n",
    "        x:feature_batch, \n",
    "        y:label_batch, \n",
    "        keep_prob:1.})\n",
    "    \n",
    "    val_acc = session.run(accuracy, feed_dict={\n",
    "        x:valid_features,\n",
    "        y:valid_labels,\n",
    "        keep_prob:1.\n",
    "    })\n",
    "    print('loss: {:>.4f} val_acc: {:.4f}'.format(loss, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数\n",
    "\n",
    "调试以下超参数：\n",
    "* 设置 `epochs` 表示神经网络停止学习或开始过拟合的迭代次数\n",
    "* 设置 `batch_size`，表示机器内存允许的部分最大体积。大部分人设为以下常见内存大小：\n",
    "\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* 设置 `keep_probability` 表示使用丢弃时保留节点的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 80\n",
    "batch_size = 128\n",
    "keep_probability = 0.42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在单个 CIFAR-10 部分上训练\n",
    "\n",
    "我们先用单个部分，而不是用所有的 CIFAR-10 批次训练神经网络。这样可以节省时间，并对模型进行迭代，以提高准确率。最终验证准确率达到 50% 或以上之后，在下一部分对所有数据运行模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss: 2.2868 val_acc: 0.1246\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss: 2.1988 val_acc: 0.2076\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss: 2.1370 val_acc: 0.2754\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss: 2.1104 val_acc: 0.3058\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss: 2.0768 val_acc: 0.3420\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss: 2.0492 val_acc: 0.3642\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss: 2.0193 val_acc: 0.3742\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss: 1.9835 val_acc: 0.3828\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss: 1.9562 val_acc: 0.3894\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss: 1.9307 val_acc: 0.3996\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss: 1.9005 val_acc: 0.4052\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss: 1.8724 val_acc: 0.4112\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss: 1.8369 val_acc: 0.4138\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss: 1.8100 val_acc: 0.4198\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss: 1.7873 val_acc: 0.4260\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss: 1.7543 val_acc: 0.4268\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss: 1.7266 val_acc: 0.4322\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss: 1.7060 val_acc: 0.4384\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss: 1.6882 val_acc: 0.4392\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss: 1.6669 val_acc: 0.4446\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss: 1.6444 val_acc: 0.4436\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss: 1.6291 val_acc: 0.4472\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss: 1.6196 val_acc: 0.4498\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss: 1.6022 val_acc: 0.4500\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss: 1.5826 val_acc: 0.4520\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss: 1.5684 val_acc: 0.4538\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss: 1.5559 val_acc: 0.4540\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss: 1.5414 val_acc: 0.4562\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss: 1.5307 val_acc: 0.4606\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss: 1.5143 val_acc: 0.4626\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss: 1.4975 val_acc: 0.4606\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss: 1.4911 val_acc: 0.4644\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss: 1.4737 val_acc: 0.4656\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss: 1.4752 val_acc: 0.4672\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss: 1.4566 val_acc: 0.4710\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss: 1.4405 val_acc: 0.4718\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss: 1.4439 val_acc: 0.4742\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss: 1.4283 val_acc: 0.4760\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss: 1.4150 val_acc: 0.4776\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss: 1.4138 val_acc: 0.4804\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss: 1.3949 val_acc: 0.4810\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss: 1.3867 val_acc: 0.4842\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss: 1.3836 val_acc: 0.4842\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss: 1.3751 val_acc: 0.4886\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss: 1.3675 val_acc: 0.4890\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss: 1.3551 val_acc: 0.4902\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss: 1.3496 val_acc: 0.4936\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss: 1.3413 val_acc: 0.4952\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss: 1.3294 val_acc: 0.4912\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss: 1.3352 val_acc: 0.4966\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss: 1.3269 val_acc: 0.4984\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss: 1.3139 val_acc: 0.4992\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss: 1.3033 val_acc: 0.4944\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss: 1.2980 val_acc: 0.4948\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss: 1.2942 val_acc: 0.4966\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss: 1.2739 val_acc: 0.4948\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss: 1.2713 val_acc: 0.4996\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss: 1.2642 val_acc: 0.4998\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss: 1.2616 val_acc: 0.5020\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss: 1.2604 val_acc: 0.5000\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss: 1.2508 val_acc: 0.5018\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss: 1.2386 val_acc: 0.5002\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss: 1.2334 val_acc: 0.5024\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss: 1.2435 val_acc: 0.5016\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss: 1.2188 val_acc: 0.5024\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss: 1.2200 val_acc: 0.4986\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss: 1.2001 val_acc: 0.5018\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss: 1.1931 val_acc: 0.5028\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss: 1.1967 val_acc: 0.5026\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss: 1.1850 val_acc: 0.5028\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss: 1.1828 val_acc: 0.5016\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss: 1.1855 val_acc: 0.5046\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss: 1.1583 val_acc: 0.5052\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss: 1.1756 val_acc: 0.5046\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss: 1.1593 val_acc: 0.5082\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss: 1.1614 val_acc: 0.5070\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss: 1.1402 val_acc: 0.5062\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss: 1.1444 val_acc: 0.5084\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss: 1.1341 val_acc: 0.5080\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss: 1.1301 val_acc: 0.5082\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完全训练模型\n",
    "\n",
    "现在，单个 CIFAR-10 部分的准确率已经不错了，试试所有五个部分吧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss: 2.2006 val_acc: 0.1774\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss: 2.1396 val_acc: 0.2574\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss: 1.8480 val_acc: 0.3008\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss: 1.8993 val_acc: 0.3406\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss: 1.8717 val_acc: 0.3560\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss: 2.0237 val_acc: 0.3850\n",
      "Epoch  2, CIFAR-10 Batch 2:  loss: 1.8794 val_acc: 0.3820\n",
      "Epoch  2, CIFAR-10 Batch 3:  loss: 1.5869 val_acc: 0.3954\n",
      "Epoch  2, CIFAR-10 Batch 4:  loss: 1.6762 val_acc: 0.4006\n",
      "Epoch  2, CIFAR-10 Batch 5:  loss: 1.7533 val_acc: 0.4064\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss: 1.8897 val_acc: 0.4248\n",
      "Epoch  3, CIFAR-10 Batch 2:  loss: 1.7321 val_acc: 0.4202\n",
      "Epoch  3, CIFAR-10 Batch 3:  loss: 1.3683 val_acc: 0.4306\n",
      "Epoch  3, CIFAR-10 Batch 4:  loss: 1.5964 val_acc: 0.4346\n",
      "Epoch  3, CIFAR-10 Batch 5:  loss: 1.6346 val_acc: 0.4364\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss: 1.7881 val_acc: 0.4442\n",
      "Epoch  4, CIFAR-10 Batch 2:  loss: 1.6502 val_acc: 0.4460\n",
      "Epoch  4, CIFAR-10 Batch 3:  loss: 1.2564 val_acc: 0.4432\n",
      "Epoch  4, CIFAR-10 Batch 4:  loss: 1.5540 val_acc: 0.4510\n",
      "Epoch  4, CIFAR-10 Batch 5:  loss: 1.5786 val_acc: 0.4566\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss: 1.7124 val_acc: 0.4590\n",
      "Epoch  5, CIFAR-10 Batch 2:  loss: 1.5709 val_acc: 0.4530\n",
      "Epoch  5, CIFAR-10 Batch 3:  loss: 1.2061 val_acc: 0.4598\n",
      "Epoch  5, CIFAR-10 Batch 4:  loss: 1.4930 val_acc: 0.4640\n",
      "Epoch  5, CIFAR-10 Batch 5:  loss: 1.5513 val_acc: 0.4562\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss: 1.6758 val_acc: 0.4690\n",
      "Epoch  6, CIFAR-10 Batch 2:  loss: 1.4894 val_acc: 0.4620\n",
      "Epoch  6, CIFAR-10 Batch 3:  loss: 1.1716 val_acc: 0.4750\n",
      "Epoch  6, CIFAR-10 Batch 4:  loss: 1.4445 val_acc: 0.4734\n",
      "Epoch  6, CIFAR-10 Batch 5:  loss: 1.5167 val_acc: 0.4718\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss: 1.6458 val_acc: 0.4732\n",
      "Epoch  7, CIFAR-10 Batch 2:  loss: 1.4179 val_acc: 0.4698\n",
      "Epoch  7, CIFAR-10 Batch 3:  loss: 1.1248 val_acc: 0.4820\n",
      "Epoch  7, CIFAR-10 Batch 4:  loss: 1.4008 val_acc: 0.4802\n",
      "Epoch  7, CIFAR-10 Batch 5:  loss: 1.4838 val_acc: 0.4796\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss: 1.6201 val_acc: 0.4804\n",
      "Epoch  8, CIFAR-10 Batch 2:  loss: 1.3724 val_acc: 0.4760\n",
      "Epoch  8, CIFAR-10 Batch 3:  loss: 1.0932 val_acc: 0.4892\n",
      "Epoch  8, CIFAR-10 Batch 4:  loss: 1.3918 val_acc: 0.4860\n",
      "Epoch  8, CIFAR-10 Batch 5:  loss: 1.4567 val_acc: 0.4890\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss: 1.5912 val_acc: 0.4864\n",
      "Epoch  9, CIFAR-10 Batch 2:  loss: 1.3255 val_acc: 0.4924\n",
      "Epoch  9, CIFAR-10 Batch 3:  loss: 1.0671 val_acc: 0.4946\n",
      "Epoch  9, CIFAR-10 Batch 4:  loss: 1.3618 val_acc: 0.4988\n",
      "Epoch  9, CIFAR-10 Batch 5:  loss: 1.4069 val_acc: 0.4952\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss: 1.5518 val_acc: 0.4966\n",
      "Epoch 10, CIFAR-10 Batch 2:  loss: 1.2925 val_acc: 0.4960\n",
      "Epoch 10, CIFAR-10 Batch 3:  loss: 1.0300 val_acc: 0.5056\n",
      "Epoch 10, CIFAR-10 Batch 4:  loss: 1.3483 val_acc: 0.5106\n",
      "Epoch 10, CIFAR-10 Batch 5:  loss: 1.3890 val_acc: 0.5034\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss: 1.5160 val_acc: 0.5102\n",
      "Epoch 11, CIFAR-10 Batch 2:  loss: 1.2314 val_acc: 0.5068\n",
      "Epoch 11, CIFAR-10 Batch 3:  loss: 1.0025 val_acc: 0.5154\n",
      "Epoch 11, CIFAR-10 Batch 4:  loss: 1.3146 val_acc: 0.5160\n",
      "Epoch 11, CIFAR-10 Batch 5:  loss: 1.3461 val_acc: 0.5112\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss: 1.4873 val_acc: 0.5106\n",
      "Epoch 12, CIFAR-10 Batch 2:  loss: 1.2102 val_acc: 0.5136\n",
      "Epoch 12, CIFAR-10 Batch 3:  loss: 0.9714 val_acc: 0.5180\n",
      "Epoch 12, CIFAR-10 Batch 4:  loss: 1.2855 val_acc: 0.5224\n",
      "Epoch 12, CIFAR-10 Batch 5:  loss: 1.3006 val_acc: 0.5138\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss: 1.4435 val_acc: 0.5208\n",
      "Epoch 13, CIFAR-10 Batch 2:  loss: 1.1673 val_acc: 0.5268\n",
      "Epoch 13, CIFAR-10 Batch 3:  loss: 0.9592 val_acc: 0.5262\n",
      "Epoch 13, CIFAR-10 Batch 4:  loss: 1.2571 val_acc: 0.5280\n",
      "Epoch 13, CIFAR-10 Batch 5:  loss: 1.2637 val_acc: 0.5282\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss: 1.4117 val_acc: 0.5242\n",
      "Epoch 14, CIFAR-10 Batch 2:  loss: 1.1338 val_acc: 0.5360\n",
      "Epoch 14, CIFAR-10 Batch 3:  loss: 0.9318 val_acc: 0.5342\n",
      "Epoch 14, CIFAR-10 Batch 4:  loss: 1.2361 val_acc: 0.5394\n",
      "Epoch 14, CIFAR-10 Batch 5:  loss: 1.2339 val_acc: 0.5336\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss: 1.3892 val_acc: 0.5386\n",
      "Epoch 15, CIFAR-10 Batch 2:  loss: 1.1159 val_acc: 0.5400\n",
      "Epoch 15, CIFAR-10 Batch 3:  loss: 0.9077 val_acc: 0.5390\n",
      "Epoch 15, CIFAR-10 Batch 4:  loss: 1.1926 val_acc: 0.5448\n",
      "Epoch 15, CIFAR-10 Batch 5:  loss: 1.2000 val_acc: 0.5418\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss: 1.3635 val_acc: 0.5470\n",
      "Epoch 16, CIFAR-10 Batch 2:  loss: 1.0998 val_acc: 0.5454\n",
      "Epoch 16, CIFAR-10 Batch 3:  loss: 0.8793 val_acc: 0.5476\n",
      "Epoch 16, CIFAR-10 Batch 4:  loss: 1.1856 val_acc: 0.5488\n",
      "Epoch 16, CIFAR-10 Batch 5:  loss: 1.1725 val_acc: 0.5502\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss: 1.3236 val_acc: 0.5550\n",
      "Epoch 17, CIFAR-10 Batch 2:  loss: 1.0715 val_acc: 0.5546\n",
      "Epoch 17, CIFAR-10 Batch 3:  loss: 0.8705 val_acc: 0.5554\n",
      "Epoch 17, CIFAR-10 Batch 4:  loss: 1.1574 val_acc: 0.5484\n",
      "Epoch 17, CIFAR-10 Batch 5:  loss: 1.1439 val_acc: 0.5560\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss: 1.3110 val_acc: 0.5576\n",
      "Epoch 18, CIFAR-10 Batch 2:  loss: 1.0636 val_acc: 0.5580\n",
      "Epoch 18, CIFAR-10 Batch 3:  loss: 0.8372 val_acc: 0.5606\n",
      "Epoch 18, CIFAR-10 Batch 4:  loss: 1.1479 val_acc: 0.5564\n",
      "Epoch 18, CIFAR-10 Batch 5:  loss: 1.1473 val_acc: 0.5598\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss: 1.3165 val_acc: 0.5656\n",
      "Epoch 19, CIFAR-10 Batch 2:  loss: 1.0506 val_acc: 0.5628\n",
      "Epoch 19, CIFAR-10 Batch 3:  loss: 0.8431 val_acc: 0.5598\n",
      "Epoch 19, CIFAR-10 Batch 4:  loss: 1.1169 val_acc: 0.5608\n",
      "Epoch 19, CIFAR-10 Batch 5:  loss: 1.1070 val_acc: 0.5658\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss: 1.2845 val_acc: 0.5626\n",
      "Epoch 20, CIFAR-10 Batch 2:  loss: 1.0312 val_acc: 0.5668\n",
      "Epoch 20, CIFAR-10 Batch 3:  loss: 0.8205 val_acc: 0.5714\n",
      "Epoch 20, CIFAR-10 Batch 4:  loss: 1.1081 val_acc: 0.5676\n",
      "Epoch 20, CIFAR-10 Batch 5:  loss: 1.0864 val_acc: 0.5702\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss: 1.2720 val_acc: 0.5668\n",
      "Epoch 21, CIFAR-10 Batch 2:  loss: 1.0088 val_acc: 0.5690\n",
      "Epoch 21, CIFAR-10 Batch 3:  loss: 0.8068 val_acc: 0.5726\n",
      "Epoch 21, CIFAR-10 Batch 4:  loss: 1.0738 val_acc: 0.5728\n",
      "Epoch 21, CIFAR-10 Batch 5:  loss: 1.0745 val_acc: 0.5650\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss: 1.2728 val_acc: 0.5718\n",
      "Epoch 22, CIFAR-10 Batch 2:  loss: 0.9992 val_acc: 0.5760\n",
      "Epoch 22, CIFAR-10 Batch 3:  loss: 0.7964 val_acc: 0.5766\n",
      "Epoch 22, CIFAR-10 Batch 4:  loss: 1.0583 val_acc: 0.5746\n",
      "Epoch 22, CIFAR-10 Batch 5:  loss: 1.0510 val_acc: 0.5706\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss: 1.2588 val_acc: 0.5728\n",
      "Epoch 23, CIFAR-10 Batch 2:  loss: 0.9602 val_acc: 0.5782\n",
      "Epoch 23, CIFAR-10 Batch 3:  loss: 0.7757 val_acc: 0.5740\n",
      "Epoch 23, CIFAR-10 Batch 4:  loss: 1.0368 val_acc: 0.5778\n",
      "Epoch 23, CIFAR-10 Batch 5:  loss: 1.0377 val_acc: 0.5752\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss: 1.2352 val_acc: 0.5784\n",
      "Epoch 24, CIFAR-10 Batch 2:  loss: 0.9602 val_acc: 0.5840\n",
      "Epoch 24, CIFAR-10 Batch 3:  loss: 0.7630 val_acc: 0.5814\n",
      "Epoch 24, CIFAR-10 Batch 4:  loss: 1.0366 val_acc: 0.5794\n",
      "Epoch 24, CIFAR-10 Batch 5:  loss: 1.0236 val_acc: 0.5778\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss: 1.2172 val_acc: 0.5810\n",
      "Epoch 25, CIFAR-10 Batch 2:  loss: 0.9387 val_acc: 0.5858\n",
      "Epoch 25, CIFAR-10 Batch 3:  loss: 0.7660 val_acc: 0.5830\n",
      "Epoch 25, CIFAR-10 Batch 4:  loss: 1.0147 val_acc: 0.5864\n",
      "Epoch 25, CIFAR-10 Batch 5:  loss: 1.0045 val_acc: 0.5758\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss: 1.2109 val_acc: 0.5846\n",
      "Epoch 26, CIFAR-10 Batch 2:  loss: 0.9247 val_acc: 0.5896\n",
      "Epoch 26, CIFAR-10 Batch 3:  loss: 0.7529 val_acc: 0.5870\n",
      "Epoch 26, CIFAR-10 Batch 4:  loss: 1.0041 val_acc: 0.5834\n",
      "Epoch 26, CIFAR-10 Batch 5:  loss: 0.9918 val_acc: 0.5764\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss: 1.1811 val_acc: 0.5832\n",
      "Epoch 27, CIFAR-10 Batch 2:  loss: 0.9155 val_acc: 0.5918\n",
      "Epoch 27, CIFAR-10 Batch 3:  loss: 0.7277 val_acc: 0.5840\n",
      "Epoch 27, CIFAR-10 Batch 4:  loss: 0.9905 val_acc: 0.5908\n",
      "Epoch 27, CIFAR-10 Batch 5:  loss: 0.9841 val_acc: 0.5842\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss: 1.1730 val_acc: 0.5872\n",
      "Epoch 28, CIFAR-10 Batch 2:  loss: 0.9117 val_acc: 0.5936\n",
      "Epoch 28, CIFAR-10 Batch 3:  loss: 0.7183 val_acc: 0.5910\n",
      "Epoch 28, CIFAR-10 Batch 4:  loss: 0.9812 val_acc: 0.5944\n",
      "Epoch 28, CIFAR-10 Batch 5:  loss: 0.9893 val_acc: 0.5844\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss: 1.1606 val_acc: 0.5862\n",
      "Epoch 29, CIFAR-10 Batch 2:  loss: 0.9062 val_acc: 0.5992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, CIFAR-10 Batch 3:  loss: 0.7049 val_acc: 0.5996\n",
      "Epoch 29, CIFAR-10 Batch 4:  loss: 0.9698 val_acc: 0.5950\n",
      "Epoch 29, CIFAR-10 Batch 5:  loss: 0.9733 val_acc: 0.5826\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss: 1.1543 val_acc: 0.5924\n",
      "Epoch 30, CIFAR-10 Batch 2:  loss: 0.9086 val_acc: 0.5970\n",
      "Epoch 30, CIFAR-10 Batch 3:  loss: 0.6890 val_acc: 0.5972\n",
      "Epoch 30, CIFAR-10 Batch 4:  loss: 0.9441 val_acc: 0.6004\n",
      "Epoch 30, CIFAR-10 Batch 5:  loss: 0.9488 val_acc: 0.5876\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss: 1.1246 val_acc: 0.5952\n",
      "Epoch 31, CIFAR-10 Batch 2:  loss: 0.8867 val_acc: 0.6012\n",
      "Epoch 31, CIFAR-10 Batch 3:  loss: 0.6709 val_acc: 0.6020\n",
      "Epoch 31, CIFAR-10 Batch 4:  loss: 0.9219 val_acc: 0.5986\n",
      "Epoch 31, CIFAR-10 Batch 5:  loss: 0.9566 val_acc: 0.5882\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss: 1.1094 val_acc: 0.5946\n",
      "Epoch 32, CIFAR-10 Batch 2:  loss: 0.8595 val_acc: 0.6056\n",
      "Epoch 32, CIFAR-10 Batch 3:  loss: 0.6595 val_acc: 0.6008\n",
      "Epoch 32, CIFAR-10 Batch 4:  loss: 0.9297 val_acc: 0.6084\n",
      "Epoch 32, CIFAR-10 Batch 5:  loss: 0.9025 val_acc: 0.6014\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss: 1.1109 val_acc: 0.5950\n",
      "Epoch 33, CIFAR-10 Batch 2:  loss: 0.8729 val_acc: 0.6042\n",
      "Epoch 33, CIFAR-10 Batch 3:  loss: 0.6696 val_acc: 0.6074\n",
      "Epoch 33, CIFAR-10 Batch 4:  loss: 0.9144 val_acc: 0.6096\n",
      "Epoch 33, CIFAR-10 Batch 5:  loss: 0.9083 val_acc: 0.5938\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss: 1.0740 val_acc: 0.6014\n",
      "Epoch 34, CIFAR-10 Batch 2:  loss: 0.8534 val_acc: 0.6044\n",
      "Epoch 34, CIFAR-10 Batch 3:  loss: 0.6451 val_acc: 0.6072\n",
      "Epoch 34, CIFAR-10 Batch 4:  loss: 0.8797 val_acc: 0.6104\n",
      "Epoch 34, CIFAR-10 Batch 5:  loss: 0.8910 val_acc: 0.5966\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss: 1.0607 val_acc: 0.6076\n",
      "Epoch 35, CIFAR-10 Batch 2:  loss: 0.8378 val_acc: 0.6092\n",
      "Epoch 35, CIFAR-10 Batch 3:  loss: 0.6407 val_acc: 0.6088\n",
      "Epoch 35, CIFAR-10 Batch 4:  loss: 0.8846 val_acc: 0.6108\n",
      "Epoch 35, CIFAR-10 Batch 5:  loss: 0.8646 val_acc: 0.6092\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss: 1.0716 val_acc: 0.6036\n",
      "Epoch 36, CIFAR-10 Batch 2:  loss: 0.8142 val_acc: 0.6070\n",
      "Epoch 36, CIFAR-10 Batch 3:  loss: 0.6288 val_acc: 0.6106\n",
      "Epoch 36, CIFAR-10 Batch 4:  loss: 0.8516 val_acc: 0.6132\n",
      "Epoch 36, CIFAR-10 Batch 5:  loss: 0.8782 val_acc: 0.5970\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss: 1.0641 val_acc: 0.6080\n",
      "Epoch 37, CIFAR-10 Batch 2:  loss: 0.8201 val_acc: 0.6112\n",
      "Epoch 37, CIFAR-10 Batch 3:  loss: 0.6130 val_acc: 0.6144\n",
      "Epoch 37, CIFAR-10 Batch 4:  loss: 0.8470 val_acc: 0.6152\n",
      "Epoch 37, CIFAR-10 Batch 5:  loss: 0.8800 val_acc: 0.5978\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss: 1.0498 val_acc: 0.6072\n",
      "Epoch 38, CIFAR-10 Batch 2:  loss: 0.7984 val_acc: 0.6078\n",
      "Epoch 38, CIFAR-10 Batch 3:  loss: 0.6193 val_acc: 0.6170\n",
      "Epoch 38, CIFAR-10 Batch 4:  loss: 0.8377 val_acc: 0.6170\n",
      "Epoch 38, CIFAR-10 Batch 5:  loss: 0.8635 val_acc: 0.6042\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss: 1.0389 val_acc: 0.6078\n",
      "Epoch 39, CIFAR-10 Batch 2:  loss: 0.8047 val_acc: 0.6114\n",
      "Epoch 39, CIFAR-10 Batch 3:  loss: 0.6229 val_acc: 0.6238\n",
      "Epoch 39, CIFAR-10 Batch 4:  loss: 0.8095 val_acc: 0.6132\n",
      "Epoch 39, CIFAR-10 Batch 5:  loss: 0.8513 val_acc: 0.6076\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss: 1.0294 val_acc: 0.6092\n",
      "Epoch 40, CIFAR-10 Batch 2:  loss: 0.7838 val_acc: 0.6168\n",
      "Epoch 40, CIFAR-10 Batch 3:  loss: 0.5905 val_acc: 0.6208\n",
      "Epoch 40, CIFAR-10 Batch 4:  loss: 0.8089 val_acc: 0.6200\n",
      "Epoch 40, CIFAR-10 Batch 5:  loss: 0.8280 val_acc: 0.6122\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss: 1.0332 val_acc: 0.6106\n",
      "Epoch 41, CIFAR-10 Batch 2:  loss: 0.7598 val_acc: 0.6188\n",
      "Epoch 41, CIFAR-10 Batch 3:  loss: 0.5966 val_acc: 0.6262\n",
      "Epoch 41, CIFAR-10 Batch 4:  loss: 0.8047 val_acc: 0.6192\n",
      "Epoch 41, CIFAR-10 Batch 5:  loss: 0.8254 val_acc: 0.6114\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss: 1.0322 val_acc: 0.6186\n",
      "Epoch 42, CIFAR-10 Batch 2:  loss: 0.7752 val_acc: 0.6170\n",
      "Epoch 42, CIFAR-10 Batch 3:  loss: 0.5952 val_acc: 0.6280\n",
      "Epoch 42, CIFAR-10 Batch 4:  loss: 0.7982 val_acc: 0.6190\n",
      "Epoch 42, CIFAR-10 Batch 5:  loss: 0.8096 val_acc: 0.6126\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss: 1.0222 val_acc: 0.6140\n",
      "Epoch 43, CIFAR-10 Batch 2:  loss: 0.7556 val_acc: 0.6170\n",
      "Epoch 43, CIFAR-10 Batch 3:  loss: 0.6019 val_acc: 0.6274\n",
      "Epoch 43, CIFAR-10 Batch 4:  loss: 0.7864 val_acc: 0.6236\n",
      "Epoch 43, CIFAR-10 Batch 5:  loss: 0.7858 val_acc: 0.6182\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss: 0.9812 val_acc: 0.6166\n",
      "Epoch 44, CIFAR-10 Batch 2:  loss: 0.7463 val_acc: 0.6190\n",
      "Epoch 44, CIFAR-10 Batch 3:  loss: 0.5837 val_acc: 0.6234\n",
      "Epoch 44, CIFAR-10 Batch 4:  loss: 0.7540 val_acc: 0.6198\n",
      "Epoch 44, CIFAR-10 Batch 5:  loss: 0.7917 val_acc: 0.6208\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss: 0.9834 val_acc: 0.6200\n",
      "Epoch 45, CIFAR-10 Batch 2:  loss: 0.7373 val_acc: 0.6210\n",
      "Epoch 45, CIFAR-10 Batch 3:  loss: 0.5752 val_acc: 0.6270\n",
      "Epoch 45, CIFAR-10 Batch 4:  loss: 0.7741 val_acc: 0.6202\n",
      "Epoch 45, CIFAR-10 Batch 5:  loss: 0.7689 val_acc: 0.6234\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss: 0.9732 val_acc: 0.6220\n",
      "Epoch 46, CIFAR-10 Batch 2:  loss: 0.7438 val_acc: 0.6176\n",
      "Epoch 46, CIFAR-10 Batch 3:  loss: 0.5798 val_acc: 0.6288\n",
      "Epoch 46, CIFAR-10 Batch 4:  loss: 0.7482 val_acc: 0.6286\n",
      "Epoch 46, CIFAR-10 Batch 5:  loss: 0.7826 val_acc: 0.6152\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss: 0.9695 val_acc: 0.6068\n",
      "Epoch 47, CIFAR-10 Batch 2:  loss: 0.7392 val_acc: 0.6272\n",
      "Epoch 47, CIFAR-10 Batch 3:  loss: 0.5766 val_acc: 0.6290\n",
      "Epoch 47, CIFAR-10 Batch 4:  loss: 0.7430 val_acc: 0.6230\n",
      "Epoch 47, CIFAR-10 Batch 5:  loss: 0.8044 val_acc: 0.6134\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss: 0.9655 val_acc: 0.6218\n",
      "Epoch 48, CIFAR-10 Batch 2:  loss: 0.7410 val_acc: 0.6220\n",
      "Epoch 48, CIFAR-10 Batch 3:  loss: 0.5534 val_acc: 0.6300\n",
      "Epoch 48, CIFAR-10 Batch 4:  loss: 0.7371 val_acc: 0.6272\n",
      "Epoch 48, CIFAR-10 Batch 5:  loss: 0.7692 val_acc: 0.6186\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss: 0.9423 val_acc: 0.6170\n",
      "Epoch 49, CIFAR-10 Batch 2:  loss: 0.7131 val_acc: 0.6246\n",
      "Epoch 49, CIFAR-10 Batch 3:  loss: 0.5490 val_acc: 0.6286\n",
      "Epoch 49, CIFAR-10 Batch 4:  loss: 0.7394 val_acc: 0.6284\n",
      "Epoch 49, CIFAR-10 Batch 5:  loss: 0.7630 val_acc: 0.6230\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss: 0.9372 val_acc: 0.6202\n",
      "Epoch 50, CIFAR-10 Batch 2:  loss: 0.7185 val_acc: 0.6256\n",
      "Epoch 50, CIFAR-10 Batch 3:  loss: 0.5567 val_acc: 0.6348\n",
      "Epoch 50, CIFAR-10 Batch 4:  loss: 0.7298 val_acc: 0.6306\n",
      "Epoch 50, CIFAR-10 Batch 5:  loss: 0.7642 val_acc: 0.6152\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss: 0.9322 val_acc: 0.6266\n",
      "Epoch 51, CIFAR-10 Batch 2:  loss: 0.7255 val_acc: 0.6262\n",
      "Epoch 51, CIFAR-10 Batch 3:  loss: 0.5558 val_acc: 0.6312\n",
      "Epoch 51, CIFAR-10 Batch 4:  loss: 0.7254 val_acc: 0.6324\n",
      "Epoch 51, CIFAR-10 Batch 5:  loss: 0.7328 val_acc: 0.6276\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss: 0.9493 val_acc: 0.6278\n",
      "Epoch 52, CIFAR-10 Batch 2:  loss: 0.7155 val_acc: 0.6326\n",
      "Epoch 52, CIFAR-10 Batch 3:  loss: 0.5289 val_acc: 0.6328\n",
      "Epoch 52, CIFAR-10 Batch 4:  loss: 0.7108 val_acc: 0.6290\n",
      "Epoch 52, CIFAR-10 Batch 5:  loss: 0.7456 val_acc: 0.6248\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss: 0.9051 val_acc: 0.6224\n",
      "Epoch 53, CIFAR-10 Batch 2:  loss: 0.7159 val_acc: 0.6302\n",
      "Epoch 53, CIFAR-10 Batch 3:  loss: 0.5396 val_acc: 0.6368\n",
      "Epoch 53, CIFAR-10 Batch 4:  loss: 0.6849 val_acc: 0.6308\n",
      "Epoch 53, CIFAR-10 Batch 5:  loss: 0.7343 val_acc: 0.6190\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss: 0.8984 val_acc: 0.6270\n",
      "Epoch 54, CIFAR-10 Batch 2:  loss: 0.7053 val_acc: 0.6320\n",
      "Epoch 54, CIFAR-10 Batch 3:  loss: 0.5149 val_acc: 0.6360\n",
      "Epoch 54, CIFAR-10 Batch 4:  loss: 0.6961 val_acc: 0.6314\n",
      "Epoch 54, CIFAR-10 Batch 5:  loss: 0.7218 val_acc: 0.6172\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss: 0.8975 val_acc: 0.6280\n",
      "Epoch 55, CIFAR-10 Batch 2:  loss: 0.6990 val_acc: 0.6326\n",
      "Epoch 55, CIFAR-10 Batch 3:  loss: 0.5172 val_acc: 0.6324\n",
      "Epoch 55, CIFAR-10 Batch 4:  loss: 0.7118 val_acc: 0.6332\n",
      "Epoch 55, CIFAR-10 Batch 5:  loss: 0.7332 val_acc: 0.6194\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss: 0.8774 val_acc: 0.6294\n",
      "Epoch 56, CIFAR-10 Batch 2:  loss: 0.7122 val_acc: 0.6306\n",
      "Epoch 56, CIFAR-10 Batch 3:  loss: 0.5109 val_acc: 0.6360\n",
      "Epoch 56, CIFAR-10 Batch 4:  loss: 0.6723 val_acc: 0.6334\n",
      "Epoch 56, CIFAR-10 Batch 5:  loss: 0.7518 val_acc: 0.6274\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss: 0.8960 val_acc: 0.6262\n",
      "Epoch 57, CIFAR-10 Batch 2:  loss: 0.7025 val_acc: 0.6322\n",
      "Epoch 57, CIFAR-10 Batch 3:  loss: 0.4979 val_acc: 0.6322\n",
      "Epoch 57, CIFAR-10 Batch 4:  loss: 0.6680 val_acc: 0.6322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57, CIFAR-10 Batch 5:  loss: 0.7080 val_acc: 0.6290\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss: 0.8720 val_acc: 0.6296\n",
      "Epoch 58, CIFAR-10 Batch 2:  loss: 0.6884 val_acc: 0.6302\n",
      "Epoch 58, CIFAR-10 Batch 3:  loss: 0.5055 val_acc: 0.6396\n",
      "Epoch 58, CIFAR-10 Batch 4:  loss: 0.6753 val_acc: 0.6328\n",
      "Epoch 58, CIFAR-10 Batch 5:  loss: 0.7120 val_acc: 0.6272\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss: 0.8539 val_acc: 0.6194\n",
      "Epoch 59, CIFAR-10 Batch 2:  loss: 0.6804 val_acc: 0.6334\n",
      "Epoch 59, CIFAR-10 Batch 3:  loss: 0.5034 val_acc: 0.6356\n",
      "Epoch 59, CIFAR-10 Batch 4:  loss: 0.6753 val_acc: 0.6348\n",
      "Epoch 59, CIFAR-10 Batch 5:  loss: 0.6881 val_acc: 0.6320\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss: 0.8559 val_acc: 0.6276\n",
      "Epoch 60, CIFAR-10 Batch 2:  loss: 0.6924 val_acc: 0.6376\n",
      "Epoch 60, CIFAR-10 Batch 3:  loss: 0.4918 val_acc: 0.6360\n",
      "Epoch 60, CIFAR-10 Batch 4:  loss: 0.6594 val_acc: 0.6268\n",
      "Epoch 60, CIFAR-10 Batch 5:  loss: 0.6987 val_acc: 0.6228\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss: 0.8618 val_acc: 0.6312\n",
      "Epoch 61, CIFAR-10 Batch 2:  loss: 0.6676 val_acc: 0.6368\n",
      "Epoch 61, CIFAR-10 Batch 3:  loss: 0.5032 val_acc: 0.6278\n",
      "Epoch 61, CIFAR-10 Batch 4:  loss: 0.6554 val_acc: 0.6334\n",
      "Epoch 61, CIFAR-10 Batch 5:  loss: 0.6932 val_acc: 0.6284\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss: 0.8563 val_acc: 0.6388\n",
      "Epoch 62, CIFAR-10 Batch 2:  loss: 0.6644 val_acc: 0.6374\n",
      "Epoch 62, CIFAR-10 Batch 3:  loss: 0.4847 val_acc: 0.6338\n",
      "Epoch 62, CIFAR-10 Batch 4:  loss: 0.6097 val_acc: 0.6302\n",
      "Epoch 62, CIFAR-10 Batch 5:  loss: 0.6815 val_acc: 0.6296\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss: 0.8443 val_acc: 0.6330\n",
      "Epoch 63, CIFAR-10 Batch 2:  loss: 0.6643 val_acc: 0.6398\n",
      "Epoch 63, CIFAR-10 Batch 3:  loss: 0.4845 val_acc: 0.6374\n",
      "Epoch 63, CIFAR-10 Batch 4:  loss: 0.6282 val_acc: 0.6356\n",
      "Epoch 63, CIFAR-10 Batch 5:  loss: 0.6724 val_acc: 0.6302\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss: 0.8459 val_acc: 0.6338\n",
      "Epoch 64, CIFAR-10 Batch 2:  loss: 0.6504 val_acc: 0.6372\n",
      "Epoch 64, CIFAR-10 Batch 3:  loss: 0.4883 val_acc: 0.6434\n",
      "Epoch 64, CIFAR-10 Batch 4:  loss: 0.6286 val_acc: 0.6340\n",
      "Epoch 64, CIFAR-10 Batch 5:  loss: 0.6738 val_acc: 0.6290\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss: 0.8516 val_acc: 0.6370\n",
      "Epoch 65, CIFAR-10 Batch 2:  loss: 0.6479 val_acc: 0.6438\n",
      "Epoch 65, CIFAR-10 Batch 3:  loss: 0.4830 val_acc: 0.6388\n",
      "Epoch 65, CIFAR-10 Batch 4:  loss: 0.6312 val_acc: 0.6394\n",
      "Epoch 65, CIFAR-10 Batch 5:  loss: 0.6737 val_acc: 0.6302\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss: 0.8182 val_acc: 0.6348\n",
      "Epoch 66, CIFAR-10 Batch 2:  loss: 0.6391 val_acc: 0.6444\n",
      "Epoch 66, CIFAR-10 Batch 3:  loss: 0.4988 val_acc: 0.6336\n",
      "Epoch 66, CIFAR-10 Batch 4:  loss: 0.6416 val_acc: 0.6330\n",
      "Epoch 66, CIFAR-10 Batch 5:  loss: 0.6513 val_acc: 0.6310\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss: 0.8202 val_acc: 0.6378\n",
      "Epoch 67, CIFAR-10 Batch 2:  loss: 0.6429 val_acc: 0.6460\n",
      "Epoch 67, CIFAR-10 Batch 3:  loss: 0.4775 val_acc: 0.6406\n",
      "Epoch 67, CIFAR-10 Batch 4:  loss: 0.6235 val_acc: 0.6356\n",
      "Epoch 67, CIFAR-10 Batch 5:  loss: 0.6569 val_acc: 0.6284\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss: 0.7780 val_acc: 0.6328\n",
      "Epoch 68, CIFAR-10 Batch 2:  loss: 0.6468 val_acc: 0.6448\n",
      "Epoch 68, CIFAR-10 Batch 3:  loss: 0.4664 val_acc: 0.6420\n",
      "Epoch 68, CIFAR-10 Batch 4:  loss: 0.6079 val_acc: 0.6408\n",
      "Epoch 68, CIFAR-10 Batch 5:  loss: 0.6575 val_acc: 0.6332\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss: 0.7779 val_acc: 0.6340\n",
      "Epoch 69, CIFAR-10 Batch 2:  loss: 0.6299 val_acc: 0.6494\n",
      "Epoch 69, CIFAR-10 Batch 3:  loss: 0.4657 val_acc: 0.6420\n",
      "Epoch 69, CIFAR-10 Batch 4:  loss: 0.6143 val_acc: 0.6410\n",
      "Epoch 69, CIFAR-10 Batch 5:  loss: 0.6210 val_acc: 0.6386\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss: 0.7692 val_acc: 0.6402\n",
      "Epoch 70, CIFAR-10 Batch 2:  loss: 0.6128 val_acc: 0.6458\n",
      "Epoch 70, CIFAR-10 Batch 3:  loss: 0.4684 val_acc: 0.6430\n",
      "Epoch 70, CIFAR-10 Batch 4:  loss: 0.6056 val_acc: 0.6408\n",
      "Epoch 70, CIFAR-10 Batch 5:  loss: 0.6386 val_acc: 0.6302\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss: 0.7838 val_acc: 0.6392\n",
      "Epoch 71, CIFAR-10 Batch 2:  loss: 0.6027 val_acc: 0.6494\n",
      "Epoch 71, CIFAR-10 Batch 3:  loss: 0.4555 val_acc: 0.6434\n",
      "Epoch 71, CIFAR-10 Batch 4:  loss: 0.6157 val_acc: 0.6392\n",
      "Epoch 71, CIFAR-10 Batch 5:  loss: 0.6371 val_acc: 0.6242\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss: 0.7778 val_acc: 0.6354\n",
      "Epoch 72, CIFAR-10 Batch 2:  loss: 0.6069 val_acc: 0.6438\n",
      "Epoch 72, CIFAR-10 Batch 3:  loss: 0.4542 val_acc: 0.6448\n",
      "Epoch 72, CIFAR-10 Batch 4:  loss: 0.6025 val_acc: 0.6394\n",
      "Epoch 72, CIFAR-10 Batch 5:  loss: 0.6494 val_acc: 0.6268\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss: 0.7793 val_acc: 0.6350\n",
      "Epoch 73, CIFAR-10 Batch 2:  loss: 0.6278 val_acc: 0.6480\n",
      "Epoch 73, CIFAR-10 Batch 3:  loss: 0.4489 val_acc: 0.6400\n",
      "Epoch 73, CIFAR-10 Batch 4:  loss: 0.5962 val_acc: 0.6368\n",
      "Epoch 73, CIFAR-10 Batch 5:  loss: 0.6269 val_acc: 0.6314\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss: 0.7704 val_acc: 0.6410\n",
      "Epoch 74, CIFAR-10 Batch 2:  loss: 0.6189 val_acc: 0.6490\n",
      "Epoch 74, CIFAR-10 Batch 3:  loss: 0.4661 val_acc: 0.6458\n",
      "Epoch 74, CIFAR-10 Batch 4:  loss: 0.5954 val_acc: 0.6384\n",
      "Epoch 74, CIFAR-10 Batch 5:  loss: 0.6283 val_acc: 0.6316\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss: 0.7551 val_acc: 0.6368\n",
      "Epoch 75, CIFAR-10 Batch 2:  loss: 0.5841 val_acc: 0.6428\n",
      "Epoch 75, CIFAR-10 Batch 3:  loss: 0.4418 val_acc: 0.6458\n",
      "Epoch 75, CIFAR-10 Batch 4:  loss: 0.5886 val_acc: 0.6398\n",
      "Epoch 75, CIFAR-10 Batch 5:  loss: 0.6382 val_acc: 0.6334\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss: 0.7575 val_acc: 0.6366\n",
      "Epoch 76, CIFAR-10 Batch 2:  loss: 0.5991 val_acc: 0.6498\n",
      "Epoch 76, CIFAR-10 Batch 3:  loss: 0.4282 val_acc: 0.6452\n",
      "Epoch 76, CIFAR-10 Batch 4:  loss: 0.5845 val_acc: 0.6406\n",
      "Epoch 76, CIFAR-10 Batch 5:  loss: 0.6170 val_acc: 0.6344\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss: 0.7760 val_acc: 0.6474\n",
      "Epoch 77, CIFAR-10 Batch 2:  loss: 0.5912 val_acc: 0.6460\n",
      "Epoch 77, CIFAR-10 Batch 3:  loss: 0.4382 val_acc: 0.6452\n",
      "Epoch 77, CIFAR-10 Batch 4:  loss: 0.5824 val_acc: 0.6450\n",
      "Epoch 77, CIFAR-10 Batch 5:  loss: 0.6143 val_acc: 0.6344\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss: 0.7509 val_acc: 0.6442\n",
      "Epoch 78, CIFAR-10 Batch 2:  loss: 0.5907 val_acc: 0.6512\n",
      "Epoch 78, CIFAR-10 Batch 3:  loss: 0.4496 val_acc: 0.6316\n",
      "Epoch 78, CIFAR-10 Batch 4:  loss: 0.5892 val_acc: 0.6380\n",
      "Epoch 78, CIFAR-10 Batch 5:  loss: 0.6034 val_acc: 0.6424\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss: 0.7431 val_acc: 0.6428\n",
      "Epoch 79, CIFAR-10 Batch 2:  loss: 0.5786 val_acc: 0.6496\n",
      "Epoch 79, CIFAR-10 Batch 3:  loss: 0.4390 val_acc: 0.6396\n",
      "Epoch 79, CIFAR-10 Batch 4:  loss: 0.5644 val_acc: 0.6454\n",
      "Epoch 79, CIFAR-10 Batch 5:  loss: 0.6346 val_acc: 0.6314\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss: 0.7277 val_acc: 0.6420\n",
      "Epoch 80, CIFAR-10 Batch 2:  loss: 0.5658 val_acc: 0.6496\n",
      "Epoch 80, CIFAR-10 Batch 3:  loss: 0.4258 val_acc: 0.6426\n",
      "Epoch 80, CIFAR-10 Batch 4:  loss: 0.5789 val_acc: 0.6464\n",
      "Epoch 80, CIFAR-10 Batch 5:  loss: 0.6216 val_acc: 0.6352\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检查点\n",
    "\n",
    "模型已保存到本地。\n",
    "\n",
    "## 测试模型\n",
    "\n",
    "利用测试数据集测试你的模型。这将是最终的准确率。你的准确率应该高于 50%。如果没达到，请继续调整模型结构和参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.634493670886076\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqTA4liRJkgqDY0mSJKkwOJYkSZKK/x/wqtP3pomcewAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9898093ef0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为何准确率只有50-80%？\n",
    "\n",
    "你可能想问，为何准确率不能更高了？首先，对于简单的 CNN 网络来说，50% 已经不低了。纯粹猜测的准确率为10%。但是，你可能注意到有人的准确率[远远超过 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130)。这是因为我们还没有介绍所有的神经网络知识。我们还需要掌握一些其他技巧。\n",
    "\n",
    "## 提交项目\n",
    "\n",
    "提交项目时，确保先运行所有单元，然后再保存记事本。将 notebook 文件另存为“dlnd_image_classification.ipynb”，再在目录 \"File\" -> \"Download as\" 另存为 HTML 格式。请在提交的项目中包含 “helper.py” 和 “problem_unittests.py” 文件。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
